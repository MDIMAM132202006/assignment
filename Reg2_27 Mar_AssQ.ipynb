{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35240eb-6538-4f14-9986-88b38e99835f",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\n",
    "\n",
    "ans: Linear regression model is a statistical technique used to establish linear relation or linear equation between independent and dependent variables that represent best fit of the data. R-squared, is a measure that provides information about the goodness of fit of a model. In the context of linear regression it is a statistical measure of how well the linear regression line approximates the actual data. \n",
    "\n",
    "R-squared = 1 - (Sum squared regression(SSR)/(total Sum squared(SST)))\n",
    "\n",
    "if R-squared = 1  It represent that model able to capture 100% of the variation in dependent variable for the independent variable.\n",
    "\n",
    "if R-squared = 0.83 It represent that model able to capture 83% of the variation in dependent variable for the independet variable.\n",
    "\n",
    "if R-squared = 0  It represent that model not at all able to capture the variation in dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e8a51-6e3e-4417-b051-7c6c2efd8035",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "ans. Adjusted R-squared is statistical technique to measure the performance of the model.It tells us how good our model fitted to data points. The only difference between them is Adjusted R-squared consider effect of no. of independent variable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6122d141-62c3-4ad3-94d3-a5c89633dc97",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "ans: It is particularly useful when we are working with multiple independent variables (multiple regression) and if we want to determine whether adding more predictors to the model is improving its explanatory power. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba21239-65e2-41d1-9612-fe2e6aae9716",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "\n",
    "ans: \n",
    "MAE :  The Mean absolute error represents the average of the absolute difference between the actual and predicted values in the dataset. It measures the average of the residuals in the dataset.\n",
    "\n",
    "$$MAE = (1/N) * \\sum \\limits_{i=1} ^ {N} |y_{i} - \\hat{y}|$$\n",
    "\n",
    "MSE : Mean Squared Error represents the average of the squared difference between the original and predicted values in the data set. It measures the variance of the residuals.\n",
    "\n",
    "$$MSE = (1/N) * \\sum \\limits_{i=1} ^ {N} (y_{i} - \\hat{y})^{2}$$\n",
    "\n",
    "RMSE : Root Mean Squared Error is the square root of Mean Squared error. It measures the standard deviation of residuals.\n",
    "\n",
    "$$RMSE = \\sqrt { (1/N) * \\sum \\limits_{i=1} ^ {N} (y_{i} - \\hat{y})^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda80cd-cb63-4b1c-9120-6aa95f2228b3",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "\n",
    "ans: RMSE : \n",
    "\n",
    "advantages -\n",
    "\n",
    "    1. RMSE is easy to understand.\n",
    "    \n",
    "    2. RMSE does not penalize the errors as much as MSE does due to the square root\n",
    "\n",
    "disadvantages - \n",
    "\n",
    "    1. Like MSE, RMSE is dependent on the scale of the data. It increases in magnitude if the scale of the error increases.\n",
    "    \n",
    "    2. One major drawback of RMSE is its sensitivity to outliers and the outliers have to be removed for it to function properly.\n",
    "    \n",
    "MSE :\n",
    "\n",
    "advantages -\n",
    "\n",
    "    1. MSE penalizes the model for having huge errors by squaring them.\n",
    "    \n",
    "    2. It is particularly helpful in weeding out outliers with large errors from the model by putting more weight on them.\n",
    "\n",
    "disadvantages - \n",
    "\n",
    "    1. MSE is scale-dependent as its scale depends on the scale of the data. This makes it highly undesirable for comparing                different measures.\n",
    "    \n",
    "    2. When a new outlier is introduced into the data, the model will try to take in the outlier. By doing so it will produce         a different line of best fit which may cause the final results to be skewed.\n",
    "\n",
    "MAE :\n",
    "\n",
    "advantages - \n",
    "\n",
    "    1. It is an easy to calculate evaluation metric.\n",
    "    \n",
    "    2. All the errors are weighted on the same scale since absolute values are taken.\n",
    "    \n",
    "    3. It is useful if the training data has outliers as MAE does not penalize high errors caused by outliers.\n",
    "    \n",
    "    4. It provides an even measure of how well the model is performing.\n",
    "    \n",
    "disadvantages - \n",
    "\n",
    "    1. Sometimes the large errors coming from the outliers end up being treated as the same as low errors.\n",
    "    \n",
    "    2. One of the main disadvantages of MAE is that it is not differentiable at zero. Many optimization algorithms tend to          use differentiation to find the optimum value for parameters in the evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4c7dfc-f2da-4019-9b53-0588afc8bad6",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n",
    "\n",
    "ans: Regularization is used to reduce overfitting in the model. Lasso and Ridge are the two regularization technique which is also called L1 and L2 regularization respectively.\n",
    "\n",
    "Lasso regularization : It is a technique used in linear regression and machine learning to prevent overfitting and encourage simpler models by adding a penalty term to the regression equation. Adds a penalty term proportional to the sum of absolute values of coefficients . Lasso regularization is particularly useful when dealing with high-dimensional datasets with many features or predictors. \n",
    "\n",
    "1.Ridge regression does not eliminate any features but Lasso regression eliminate the reduntant features.\n",
    "\n",
    "2.Ridge regression adds a penalty term proportional to the sum of squared coefficients but Lasso regression adds a penalty term       proportional to the sum of absolute values of coefficients. \n",
    "\n",
    "\n",
    "Lasso regularization is used when there is small number of feature in our dataset and few feature has high value of coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc554e2b-046b-4dad-9038-0277f1159d28",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "\n",
    "ans: \n",
    "   \n",
    "Regularized linear models, such as Ridge and Lasso regression, help prevent overfitting in machine learning by adding a regularization term to the standard linear regression cost function. This regularization term encourages the model to have smaller coefficients, which, in turn, limits the model's ability to fit the training data too closely. Here's how regularized linear models work to prevent overfitting, along with an example:\n",
    "\n",
    "**1. Ridge (L2 Regularization):**\n",
    "Ridge regression adds a penalty term to the linear regression cost function. The cost function for Ridge is as follows:\n",
    "\n",
    "$$[L(\\beta) = \\text{MSE} + \\lambda \\sum_{i=1}^{p} \\beta_i^2]$$\n",
    "\n",
    "- $(\\beta)$ represents the coefficients.\n",
    "- p is the number of predictor variables.\n",
    "- $(\\lambda)$ controls the strength of the regularization (hyperparameter).\n",
    "\n",
    "The regularization term, $(\\lambda \\sum_{i=1}^{p} \\beta_i^2)$, penalizes the square of the coefficients. This encourages the model to keep the coefficients small, effectively discouraging large coefficient values.\n",
    "\n",
    "**2. Lasso (L1 Regularization):**\n",
    "Lasso regression, on the other hand, uses a different regularization term. The cost function for Lasso is as follows:\n",
    "\n",
    "$$[L(\\beta) = \\text{MSE} + \\lambda \\sum_{i=1}^{p} |\\beta_i|]$$\n",
    "\n",
    "- $(\\beta)$ represents the coefficients.\n",
    "- p is the number of predictor variables.\n",
    "- $(\\lambda)$ controls the strength of the regularization (hyperparameter).\n",
    "\n",
    "The regularization term, $(\\lambda \\sum_{i=1}^{p} |\\beta_i|)$, encourages sparsity by penalizing the absolute values of the coefficients. This leads to some coefficients being exactly zero, effectively excluding certain predictors from the model.\n",
    "\n",
    "**Illustrative Example:**\n",
    "\n",
    "Let's consider an example using Ridge regression to prevent overfitting. Suppose you're building a model to predict housing prices based on several features, including square footage, number of bedrooms, and number of bathrooms. Without regularization, the model might overfit the training data by capturing noise or making the coefficients very large.\n",
    "\n",
    "If you apply Ridge regression to this problem with a suitable value of \\(\\lambda\\), it will encourage the model to have smaller coefficients. This regularization effect prevents overfitting by limiting the model's ability to fit the training data too closely. As a result, the coefficients of less important features will be shrunk toward zero, reducing their impact on the predictions. This makes the model more robust and better at generalizing to unseen data.\n",
    "\n",
    "For instance, without regularization, the model might assign a very high positive coefficient to a rare and noisy feature, leading to overfitting. With Ridge regularization, that coefficient would be shrunk toward zero, preventing the model from overemphasizing that feature.\n",
    "\n",
    "In summary, regularized linear models like Ridge and Lasso are powerful tools to prevent overfitting in machine learning. They introduce a penalty for large coefficients, encouraging simplicity and reducing the risk of fitting noise in the data. By controlling the strength of the regularization, you can strike a balance between bias and variance, leading to models that generalize well to new data.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a65afa4-2a7a-4b67-87a6-df239886882b",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "\n",
    "ans: \n",
    "    \n",
    "Regularized linear models like Ridge and Lasso have limitations, including a potential loss of interpretability, biased estimates, and difficulty handling non-linear relationships. They may not be the best choice when precise coefficients are required, the relationship is non-linear, or other factors such as multicollinearity need to be addressed. The choice of whether to use regularized linear models depends on the specific characteristics of the data and the analysis goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa4ee54-bdf5-42c6-8aa3-0195644c490a",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "ans: \n",
    "    \n",
    "    Choosing between two regression models based on different evaluation metrics requires considering the characteristics of the metrics and the goals of the analysis. In this case, you are comparing Model A with an RMSE of 10 and Model B with an MAE of 8.\n",
    "\n",
    "**RMSE (Root Mean Square Error):**\n",
    "- RMSE gives higher weight to large errors because it squares the differences between predicted and actual values.\n",
    "- It penalizes outliers more than MAE, making it sensitive to extreme errors.\n",
    "- Smaller RMSE values indicate better accuracy.\n",
    "\n",
    "**MAE (Mean Absolute Error):**\n",
    "- MAE treats all errors equally and does not penalize outliers.\n",
    "- It is robust to extreme errors and is a more straightforward metric to interpret.\n",
    "- Smaller MAE values indicate better accuracy.\n",
    "\n",
    "The choice between RMSE and MAE depends on the nature of the problem and the specific goals:\n",
    "\n",
    "1. **RMSE is Suitable When:**\n",
    "   - Large errors or outliers should be penalized more.\n",
    "   - You want to emphasize the impact of errors that are further from the true values.\n",
    "   - You are okay with a metric that tends to provide a more optimistic view of model performance.\n",
    "\n",
    "2. **MAE is Suitable When:**\n",
    "   - All errors, including outliers, should be treated equally.\n",
    "   - You want a metric that is more robust to extreme errors.\n",
    "   - You prefer a more intuitive and straightforward metric.\n",
    "\n",
    "In your case, Model B has a smaller MAE (8) compared to Model A's RMSE (10). If your primary concern is to minimize the impact of extreme errors and you prefer a more robust and straightforward metric, Model B (with MAE of 8) might be the better choice. However, the decision also depends on the specific goals of your analysis.\n",
    "\n",
    "**Limitations to Consider:**\n",
    "- The choice between RMSE and MAE is not one-size-fits-all. The \"better\" metric depends on the problem and goals.\n",
    "- RMSE may not be suitable for problems where outliers or large errors are expected.\n",
    "- MAE might underemphasize the importance of accuracy when large errors are problematic.\n",
    "- It's a good practice to consider both metrics and potentially other evaluation metrics (e.g., R-squared, adjusted R-squared) to make a more informed decision.\n",
    "\n",
    "In practice, it's essential to assess the specific context of your regression problem, your tolerance for different types of errors, and your priorities when deciding which model is the better performer. Additionally, consider conducting cross-validation and checking model assumptions to ensure robust model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e32e3-1bf1-476e-84d4-93a97c92515b",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?\n",
    "\n",
    "ans:\n",
    "    \n",
    "Choosing between two regularized linear models with different types of regularization (Ridge and Lasso) and different regularization parameters involves considering the specific characteristics of each regularization method and the goals of the analysis. In this case, Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Here's how to make a decision:\n",
    "\n",
    "**Ridge Regularization:**\n",
    "- Ridge regression adds a penalty term that encourages coefficients to be small, but it does not set any coefficients exactly to zero.\n",
    "- The regularization parameter (\\(\\lambda\\)) controls the strength of the penalty. Smaller \\(\\lambda\\) values allow coefficients to vary more freely, while larger \\(\\lambda\\) values shrink coefficients toward zero.\n",
    "- Ridge is suitable for situations with multicollinearity, where several predictors are highly correlated.\n",
    "\n",
    "**Lasso Regularization:**\n",
    "- Lasso regression introduces a penalty term that not only shrinks coefficients but also sets some coefficients exactly to zero. This makes Lasso a feature selection method as well.\n",
    "- The regularization parameter (\\(\\lambda\\)) controls the strength of the penalty. Larger \\(\\lambda\\) values lead to more coefficients being set to zero.\n",
    "- Lasso is useful when you want to perform variable selection and keep only the most important predictors in the model.\n",
    "\n",
    "To choose the better performer between Model A (Ridge) and Model B (Lasso), you should consider the specific goals and the trade-offs of each method:\n",
    "\n",
    "**Choose Model A (Ridge) If:**\n",
    "- You want to keep all predictors in the model, even if they have relatively small effects.\n",
    "- You are dealing with multicollinearity, as Ridge is effective at handling correlated predictors.\n",
    "- The primary goal is to shrink coefficients without necessarily excluding any predictors.\n",
    "\n",
    "**Choose Model B (Lasso) If:**\n",
    "- You want to perform feature selection and identify the most important predictors.\n",
    "- You believe that some predictors are not relevant or want to simplify the model.\n",
    "- The primary goal is to set some coefficients to exactly zero, effectively excluding certain predictors.\n",
    "\n",
    "**Trade-Offs and Limitations:**\n",
    "- The choice between Ridge and Lasso depends on the problem and the importance of feature selection. Lasso can lead to more parsimonious models by eliminating some predictors, but Ridge retains all predictors.\n",
    "- The selection of the regularization parameter (\\(\\lambda\\)) is crucial and often requires cross-validation to find the best balance between bias and variance.\n",
    "- Both Ridge and Lasso introduce bias into the coefficient estimates, which may not always provide the most accurate individual coefficient values.\n",
    "\n",
    "In practice, it's essential to understand the specific needs and goals of your analysis. If the aim is to retain all predictors and reduce multicollinearity, Ridge may be preferred. If the goal is to identify the most important predictors and simplify the model, Lasso is a good choice. Additionally, model evaluation, cross-validation, and an understanding of the context of the problem are crucial for making an informed choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17067e-f519-4b13-99a0-512cf0689115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
